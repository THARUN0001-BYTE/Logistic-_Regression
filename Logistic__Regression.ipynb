{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1: What is Logistic Regression, and how does it differ from Linear**\n",
        "**Regression? **\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Logistic Regression is a statistical and machine learning method used for classification problems, especially when the output (dependent variable) is categorical, most commonly binary (e.g., Yes/No, Spam/Not Spam).\n",
        "\n",
        ".It predicts the probability of an outcome belonging to a certain class.\n",
        "\n",
        ".The output values range between 0 and 1 using the sigmoid (logistic) function.\n",
        "\n",
        ".A decision boundary (e.g., probability > 0.5) is applied to classify the outcome.\n",
        "\n",
        "Linear regression draws a straight line to fit data points.\n",
        "\n",
        "Logistic regression squeezes that line through a sigmoid curve so outputs stay between 0 and 1, making it suitable for classification.\n",
        "\n"
      ],
      "metadata": {
        "id": "aqD-hS48_t4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "THo_JJxGBHdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Explain the role of the Sigmoid function in Logistic Regression?**\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "In Logistic Regression, the sigmoid function (also called the logistic function) plays the role of converting a raw linear prediction into a probability between 0 and 1.\n",
        "\n",
        "The regression equation inside logistic regression is still linear:\n",
        "This\n",
        "𝑧\n",
        "z can be any real number\n",
        "(\n",
        "−\n",
        "∞\n",
        ",\n",
        "∞\n",
        ")\n",
        "(−∞,∞), but probabilities must lie between 0 and 1.\n",
        "That’s where the sigmoid comes in — it “squashes” any real number into the range\n",
        "(\n",
        "0\n",
        ",\n",
        "1\n",
        ")\n",
        "(0,1).\n",
        "\n",
        "**2.The Sigmoid Function**\n",
        "\n",
        "\n",
        "If\n",
        "𝑧\n",
        "z is very large →\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "σ(z) approaches 1.\n",
        "\n",
        "If\n",
        "𝑧\n",
        "z is very small (negative) →\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "σ(z) approaches 0.\n",
        "\n",
        "If\n",
        "𝑧\n",
        "=\n",
        "0\n",
        "z=0 →\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "0.5\n",
        "σ(z)=0.5.\n",
        "\n",
        "**3. Role in Logistic Regression**\n",
        "\n",
        "Probability Conversion\n",
        "Turns the raw score\n",
        "𝑧\n",
        "z into a probability\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "P(Y=1∣X).\n",
        "\n",
        "Decision Boundary\n",
        "If\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "≥\n",
        "0.5\n",
        "σ(z)≥0.5 → predict class 1; otherwise → class 0 (threshold can be adjusted).\n",
        "\n",
        "Interpretability\n",
        "Outputs can be directly interpreted as “chance of belonging to class 1.”\n",
        "\n",
        "**4.Intuition**\n",
        "\n",
        "Think of the sigmoid function as a soft switch:\n",
        "\n",
        "Instead of abruptly saying “yes” or “no” (like a step function), it gradually changes from 0 to 1.\n",
        "\n",
        "This smoothness makes it differentiable, which is crucial for optimization using gradient descent."
      ],
      "metadata": {
        "id": "PyDrrWP3BJD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "hmZj0DRyDqR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is Regularization in Logistic Regression and why is it needed?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Regularization in Logistic Regression is a technique used to prevent overfitting by adding a penalty term to the model’s loss function.\n",
        "It discourages the model from assigning too large weights to features, which can cause it to fit noise in the training data instead of learning general patterns.\n",
        "\n",
        "In logistic regression, if a feature strongly separates the classes, the model may give it a very large coefficient (weight).\n",
        "\n",
        "Large weights make the model sensitive to small changes in input data → overfitting.\n",
        "\n",
        "Overfitted models work well on training data but poorly on unseen data.\n",
        "\n",
        "Regularization controls the complexity of the model by shrinking weights.\n",
        "\n",
        "**3. Benefits**\n",
        "\n",
        "Reduces overfitting → better generalization to new data.\n",
        "\n",
        "Improves stability of the model.\n",
        "\n",
        "Can perform automatic feature selection (L1 case).\n",
        "\n"
      ],
      "metadata": {
        "id": "NCFcyyHkDs8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1lkkEkUi9kaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4: What are some common evaluation metrics for classification models, and**\n",
        "**why are they important?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "When we build a classification model (like Logistic Regression, Decision Trees, etc.), we need to check how well it performs — not just on training data, but on unseen data. This is where evaluation metrics come in.\n",
        "\n",
        "**1. Common Evaluation Metrics for Classification**\n",
        "\n",
        "**(a) Accuracy**\n",
        "\n",
        "Accuracy=\n",
        "Total Predictions/\n",
        "Correct Predictions\n",
        "​\n",
        "\n",
        "Example: If your model predicts correctly 90 out of 100 times → Accuracy = 90%.\n",
        "\n",
        "When useful: When classes are balanced (equal distribution of classes).\n",
        "\n",
        "Limitation: Misleading for imbalanced data (e.g., 99% predicting “not fraud” in a fraud dataset isn’t good).\n",
        "\n",
        "**(b) Precision**\n",
        "\n",
        "Precision=\n",
        "True Positives + False Positives/\n",
        "True Positives\n",
        "​\n",
        "\n",
        "Example: Out of 10 emails marked spam, 8 are truly spam → Precision = 80%.\n",
        "\n",
        "When useful: When false positives are costly (e.g., flagging legitimate emails as spam).\n",
        "\n",
        "**(c) Recall (Sensitivity / True Positive Rate)**\n",
        "\n",
        "Recall=\n",
        "True Positives + False Negatives/\n",
        "True Positives\n",
        "​\n",
        "Example: Out of 100 spam emails, your model catches 90 → Recall = 90%.\n",
        "\n",
        "When useful: When missing positives is costly (e.g., detecting cancer, fraud).\n",
        "\n",
        "**(d) F1-Score**\n",
        "\n",
        "F1=2×\n",
        "Precision + Recall/\n",
        "Precision×Recall\n",
        "​\n",
        "When useful: When you need a balance between Precision and Recall, especially in imbalanced datasets.\n",
        "\n",
        "**(e) ROC-AUC (Receiver Operating Characteristic – Area Under Curve)**\n",
        "\n",
        "Definition: Measures the model’s ability to distinguish between classes across different thresholds.\n",
        "\n",
        "AUC Value Meaning:\n",
        "\n",
        "0.5 → No better than random guessing\n",
        "\n",
        "1.0 → Perfect separation\n",
        "\n",
        "When useful: For comparing models, especially in binary classification.\n",
        "\n",
        "**(f) Confusion Matrix**\n",
        "\n",
        "Definition: A table showing True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
        "\n",
        "Why useful: Gives a full picture of what types of errors the model makes.\n",
        "\n",
        "Different problems care about different errors.\n",
        "\n",
        "Medical diagnosis → high Recall (catch all patients).\n",
        "\n",
        "Email spam filter → high Precision (don’t mark real emails as spam).\n",
        "\n",
        "Accuracy alone can be misleading when data is imbalanced.\n",
        "\n",
        "They help in model selection and hyperparameter tuning.\n",
        "\n",
        "They guide trade-offs (e.g., increasing recall might lower precision).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_N1aPG7e9lsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6b6lo5IzBy1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5: Write a Python program that loads a CSV file into a Pandas DataFrame,**\n",
        "**splits into train/test sets, trains a Logistic Regression model, and prints its** **accuracy (Use Dataset from sklearn package)?**\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Python example using Pandas and scikit-learn with the built-in Breast Cancer dataset from sklearn.\n",
        "We’ll:\n",
        "\n",
        "1.Load the dataset.\n",
        "\n",
        "2.Convert it into a Pandas DataFrame.\n",
        "\n",
        "3.Split into training/testing sets.\n",
        "\n",
        "4.Train a Logistic Regression model.\n",
        "\n",
        "5.Print accuracy on the test set.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SLj8A_KOBz45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset from sklearn\n",
        "cancer = datasets.load_breast_cancer()\n",
        "\n",
        "# Step 2: Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "df['target'] = cancer.target\n",
        "\n",
        "# Step 3: Split into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Step 4: Train/Test split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 5: Create and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)  # max_iter increased to ensure convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 7: Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 8: Print results\n",
        "print(\"Logistic Regression Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MM05SBRaER15",
        "outputId": "b84953d3-626e-4300-9dba-e4823d25b735"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Model Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IIQsc0JnFowq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6: Write a Python program to train a Logistic Regression model using L2 **\n",
        "**regularization (Ridge) and print the model coefficients and accuracy.**\n",
        "**(Use Dataset from sklearn package)?**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Here’s the Python program using L2 Regularization (Ridge) in Logistic Regression with the Breast Cancer dataset from sklearn.\n",
        "\n",
        "By default, LogisticRegression in scikit-learn uses L2 regularization, so we’ll explicitly set penalty='l2' and print the model coefficients."
      ],
      "metadata": {
        "id": "sS5CXbg1FpsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "\n",
        "# Step 2: Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "df['target'] = cancer.target\n",
        "\n",
        "# Step 3: Split into features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Step 4: Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 5: Train Logistic Regression model with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', C=1.0, max_iter=10000)  # C controls regularization strength\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 7: Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 8: Print coefficients and accuracy\n",
        "print(\"Model Coefficients:\\n\", model.coef_)\n",
        "print(\"\\nIntercept:\", model.intercept_)\n",
        "print(\"\\nAccuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mp1RQdiHXXM",
        "outputId": "1e94f3c5-cceb-4419-fddb-a971850f6c18"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients:\n",
            " [[ 1.0274368   0.22145051 -0.36213488  0.0254667  -0.15623532 -0.23771256\n",
            "  -0.53255786 -0.28369224 -0.22668189 -0.03649446 -0.09710208  1.3705667\n",
            "  -0.18140942 -0.08719575 -0.02245523  0.04736092 -0.04294784 -0.03240188\n",
            "  -0.03473732  0.01160522  0.11165329 -0.50887722 -0.01555395 -0.016857\n",
            "  -0.30773117 -0.77270908 -1.42859535 -0.51092923 -0.74689363 -0.10094404]]\n",
            "\n",
            "Intercept: [28.64871395]\n",
            "\n",
            "Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WVLzM424H7pM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7: Write a Python program to train a Logistic Regression model for multiclass\n",
        "classification using multi_class='ovr' and print the classification report.\n",
        "(Use Dataset from sklearn package)?\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "the Python program using multi_class='ovr' (One-vs-Rest) Logistic Regression on the Iris dataset from sklearn:"
      ],
      "metadata": {
        "id": "vdgpXj-rH8hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Step 1: Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Step 2: Create DataFrame\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Step 3: Features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Step 4: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 5: Create Logistic Regression model for multiclass classification\n",
        "model = LogisticRegression(multi_class='ovr', max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 7: Print classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ba43TVqYIiG7",
        "outputId": "59860c15-7b1a-4a2b-aa37-890d09804d9b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.89      0.94         9\n",
            "   virginica       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dhZAQtFxJCeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.Write a Python program to apply GridSearchCV to tune C and penalty**\n",
        "**hyperparameters for Logistic Regression and print the best parameters and** **validation accuracy.  (Use Dataset from sklearn package) ?**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Here’s the Python program that uses GridSearchCV to tune the C (regularization strength) and penalty hyperparameters for Logistic Regression using a dataset from sklearn (I’ll use the Iris dataset):"
      ],
      "metadata": {
        "id": "J0aj3yhOJDno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import datasets\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Step 2: Create DataFrame\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Step 3: Split features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Step 4: Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 5: Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],           # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],                # L1 (Lasso) or L2 (Ridge)\n",
        "    'solver': ['liblinear']                 # liblinear supports both L1 and L2\n",
        "}\n",
        "\n",
        "# Step 6: Create Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Step 7: Apply GridSearchCV\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ns2mPgT-J-09",
        "outputId": "37c7aa56-a969-4229-ca5d-30182fb9dd44"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Cross-Validation Accuracy: 0.9583333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "N0jSg_vaLnjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9: Write a Python program to standardize the features before training Logistic**\n",
        "**Regression and compare the model's accuracy with and without scaling.**\n",
        "**(Use Dataset from sklearn package)?**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        " Python program that compares Logistic Regression accuracy with and without feature standardization using the Breast Cancer dataset from sklearn:"
      ],
      "metadata": {
        "id": "iAxQNo_8LomC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "\n",
        "# Step 2: Create DataFrame\n",
        "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "df['target'] = cancer.target\n",
        "\n",
        "# Step 3: Split into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Step 4: Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ---- Model WITHOUT scaling ----\n",
        "model_no_scale = LogisticRegression(max_iter=10000)\n",
        "model_no_scale.fit(X_train, y_train)\n",
        "y_pred_no_scale = model_no_scale.predict(X_test)\n",
        "acc_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
        "\n",
        "# ---- Model WITH scaling ----\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=10000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Step 5: Print comparison\n",
        "print(\"Accuracy without scaling:\", acc_no_scale)\n",
        "print(\"Accuracy with scaling:\", acc_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yj6nQC_dMRfA",
        "outputId": "57c92682-bda1-40df-cc84-680df9e690e8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.956140350877193\n",
            "Accuracy with scaling: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "g7idGl-qMtfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10: Imagine you are working at an e-commerce company that wants to**\n",
        "**predict which customers will respond to a marketing campaign. Given an** **imbalanceddataset (only 5% of customers respond), describe the approach you’d** **take to build aLogistic Regression model — including data handling, feature** **scaling, balancingclasses, hyperparameter tuning, and evaluating the model for** **this real-world business use case. ?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "simulate an imbalanced dataset (like your 5% response rate case) using sklearn's make_classification"
      ],
      "metadata": {
        "id": "KLE_XmWPNdmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE  # to handle imbalance\n",
        "\n",
        "# Step 2: Create an imbalanced dataset (5% positive class)\n",
        "X, y = make_classification(\n",
        "    n_samples=5000, n_features=10, n_informative=6, n_redundant=2,\n",
        "    n_classes=2, weights=[0.95, 0.05], flip_y=0, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Step 4: Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 5: Handle Class Imbalance (SMOTE Oversampling)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Step 6: Hyperparameter Tuning with GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],  # L1 = Lasso, L2 = Ridge\n",
        "    'solver': ['liblinear']   # solver for L1/L2 penalties\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    LogisticRegression(max_iter=1000),\n",
        "    param_grid,\n",
        "    scoring='roc_auc',  # good for imbalanced datasets\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Step 7: Best Model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Step 8: Evaluation\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "y_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nROC-AUC Score:\", roc_auc_score(y_test, y_proba))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYR2K29xPgo-",
        "outputId": "4fd84a4d-b1bb-4d4a-baed-056c68b241fd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.76      0.86       950\n",
            "           1       0.15      0.82      0.26        50\n",
            "\n",
            "    accuracy                           0.76      1000\n",
            "   macro avg       0.57      0.79      0.56      1000\n",
            "weighted avg       0.95      0.76      0.83      1000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[721 229]\n",
            " [  9  41]]\n",
            "\n",
            "ROC-AUC Score: 0.8634947368421052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p5qT5gvWPUn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "urvlcwpGEdU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rhPr00Lk_s9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XIVhG3qA_s6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "71LNt9r3_s4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-oE2bboq_s1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "plTKtk-N_sun"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj8ZwstF-ciT"
      },
      "outputs": [],
      "source": []
    }
  ]
}